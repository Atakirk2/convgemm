/**Convolution DNN Evaluator
 * 
 * This evaluator measures the performance obtained by the  3 algorithms compared 
 * to solve convolutions into CNN convolutional layers. The evaluator compares a naive 
 * convolution against the im2col + GEMM approach against our mplicit im2col gemm (gemm_conv) 
 * algorithm. 
 * The evaluator admits a compilation parameter to omit the execution of the naive
 * convolution (-DNONAIVE).
 * The evaluator also has two evaluation modes that can be chosen at compile time:
 *  -Model evaluation(default): Times the execution of the whole CNN model.
 *  -Layer evaluation(-DLAYER_EVAL): Times each layer and then agregates the results.
 * 
 * The models used by this evaluator had been pruned to only contain the convolutional 
 * layers of the corresponding CNN.
 * The evaluation was developed and tested for ARMCortex  A-57
 * 
 * @author P. San Juan
 * @date 04/2020
 */
#include <unistd.h>
#include <stdio.h>
#include <stdlib.h>
#include "gemmConv.h"
#include "convCommon.h"

#define CONV 0
#define IM2COL_GEMM 1
#define GEMM 2
#define IMPLICIT_GEMM 3

//typedef dualBuffer;
struct dualBufferS{
    float * buff;
    struct dualBufferS* partner; 
};

struct dualBufferH{
    _Float16 * buff;
    struct dualBufferH* partner; 
};


/** Computes the maximum matrix sizes for a model.
 * 
 * Computes the sizes of all matrices for each layer of the model using the model
 * parameters and returns the maximum size for each type of matrix.
 * 
 * @param[in] model CNN model parameters.
 * @param[in] nL Number of layers of the model.
 * @param[in] maxBatch Maximum batch size.
 * @param[out] masSizeF Maximum size of Filters matrix
 * @param[out] maxSizeIn Maximum size of input tensor.
 * @param[out] maxSizeOut Maximum size of the output matrix from the convolution.
 * @param[out] maxSizeAux Maximum size of the auxiliar matrix generated by aplying the im2col transform to the input tensor.
 */
void maxMatrixSizes(int** model,const int nL, const int maxBatch, int* maxSizeF, int* maxSizeIn, int* maxSizeOut, int* maxSizeAux)
{
    int l,
    sizeF,sizeIn, sizeOut, sizeAux, ho, wo;
    
    *maxSizeF = 0; *maxSizeIn = 0; *maxSizeOut = 0;
    
    for(l=0; l < nL; l++)
    {
		//sizeF = kn* kh *kw*c  
        sizeF = model[l][6] * model[l][4] * model[l][5] * model[l][3];
		//sizeIn = h* w* c * b
        sizeIn = model[l][1] * model[l][2] * model[l][3] * maxBatch;
		
		//ho = floor((h - kh + 2 * pad) / stride + 1);
		ho = floor((model[l][1] - model[l][4] + 2 * model[l][8]) / model[l][7] + 1);
        //wo = floor((w - kw + 2 * pad) / stride + 1);
        wo = floor((model[l][2] - model[l][5] + 2 * model[l][8]) / model[l][7] + 1);
		//sizeOut = ho * wo *kn * b
        sizeOut = ho * wo * model[l][6] * maxBatch;
		//sizeAux = c*kh*kw *ho*wo*b
		sizeAux = model[l][3] * model[l][4] * model[l][5] * ho * wo * maxBatch;
        *maxSizeF = sizeF > *maxSizeF? sizeF: *maxSizeF;
        *maxSizeIn = sizeIn > *maxSizeIn? sizeIn: *maxSizeIn;
        *maxSizeOut = sizeOut  > *maxSizeOut? sizeOut: *maxSizeOut;
		*maxSizeAux = sizeAux  > *maxSizeAux? sizeAux: *maxSizeAux;
    }
    
}


/** Timing of a CNN model.
 * 
 * Executes and times a given model for a given batch size using the selected algorithm.
 * This function was developed to avoid unnecesary code replication.
 * 
 * @param[in] algorithm Algorithm to evaluate [CONV, IM2COL_GEMM, GEMM, or IMPLICIT_GEMM].
 * @param[in] model CNN model parameters.
 * @param[in] nL Number of layers of the model.
 * @param[in] repe NUmber of repetitions for timing smoothing.
 * @param[in] b Batch size.
 * @param[in] j Index to adress the corresponding batch size positions in the timer matrix.
 * @param[in] F Filter matrix.
 * @param[in] inOut Dual buffer for input/output in each layer convolution.
 * @param[in] Ac_pack  Workspace buffer for the packing of Ac.
 * @param[in] Bc_pack Workspace buffer for the packing od Bc.
 * @param[in] Aux Workspace buffer to hold the intermediate matrix generated by the im2col transform.
 * @param[in,out] timer Matrix containing the timing results for each batch size for the given algorithm.
 */
void inline timeNet(const int algorithm, int ** model, const int nL, const int repe, const int b, const int j,  float* F, struct dualBufferS* inOut,  float* Ac_pack,  float* Bc_pack,  float* Aux,  double* timer)
{
    double tIni;
    int r,l; //loop indexes 
    int h, w, c, kh, kw, kn, stride, pad, ho, wo;//convolution parameters
    float ONE=1, ZERO=0;
    
//Evaluating algorithm
#ifndef LAYER_EVAL 
        tIni = bli_clock();
        for(r = 0; r < repe; r++)
#endif
            for(l=0; l < nL; l++)
            {
                h = model[l][1]; w = model[l][2]; c = model[l][3];
                kh = model[l][4]; kw = model[l][5]; kn = model[l][6];
                stride = model[l][7]; pad = model[l][8];
                ho = floor((h - kh + 2 * pad) / stride + 1);
                wo = floor((w - kw + 2 * pad) / stride + 1);
                
#ifdef LAYER_EVAL
                tIni = bli_clock();
                for(r = 0; r < repe; r++)
#endif          
                    switch(algorithm)
                    {
                        case CONV:
                            convolutionNaive(ho,wo,c,b,inOut->buff,kh,kw,kn,F,inOut->partner->buff, stride);
                            inOut = inOut->partner;
                            break;
                        case IM2COL_GEMM:
                            im2Col(ho,wo,c,b,inOut->buff,kh,kw, stride,Aux);
                        case GEMM:
                            //bli_sgemm(BLIS_NO_TRANSPOSE,BLIS_NO_TRANSPOSE,kn,ho*wo*b,kh*kw*c,&ONE,F,1,kn,Aux,1,kh*kw*c,&ZERO,inOut->partner->buff,1,kn);
                            sgemm_cust(kn,ho*wo*b,kh*kw*c,1,F,kn,Aux,kh*kw*c,0,inOut->partner->buff,kn,Ac_pack,Bc_pack);
                            inOut = inOut->partner;
                            break;
                        case IMPLICIT_GEMM:
                            sgemm_conv(kh,kw,c,kn,1,F, ho,wo,b, stride, inOut->buff, 0,inOut->partner->buff,Ac_pack,Bc_pack);
                            inOut = inOut->partner;
                            break;
                    }
#ifdef LAYER_EVAL
                timer[l+j*(nL+2)] = bli_clock() -tIni;
#endif
                
                //bli_scopyv(BLIS_NO_CONJUGATE,kn*ho*wo*b,Out,1,Aux,1);//Copy to force cache clearing
            }
#ifndef LAYER_EVAL                 
        timer[nL+j*(nL+2)] = bli_clock() -tIni;
#endif
    
}



/** Performs a convolution evaluation of a DNN model.
 * 
 * Evaluates all the layers on the model for each convolution algorithm tested
 * and computes the timing measures.
 * 
 * @param[in] model CNN model parameters.
 * @param[in] nL Number of layers of the model.
 * @param[in] minBatch Minimum batch size to evaluate.
 * @param[in] maxBatch Max batch size to evaluate.
 * @param[in] stepBatch Step for the batch size range.
 * @param[in] repe Number of repetitios for timing smoothing.
 * @return Matrix stack containing all the timing measurments( timing_measures * (nL * numbatch)).
 */
double ** evalNet(int** model, const int nL, const int minBatch, const int maxBatch, const int stepBatch, const int repe)
{
    double **times, tIni, *tConv, *tIm2Col, *tGemm, *tIm2ColGemm, *tImp; //Timing vectors
    double sum;
    int i, j,r,l, b, //loop indexes
     	maxSizeF, maxSizeIn, maxSizeOut, maxSizeAux;//matrixz max sizes
    int numBatch, timers=5;
    float *F, *Ac_pack, *Bc_pack, *Aux;
    struct dualBufferS inOut;
    
	
	printf("Starting evaluation\n");
    
    //Allocating matrices
    maxMatrixSizes(model, nL, maxBatch, &maxSizeF, &maxSizeIn,&maxSizeOut, &maxSizeAux);

    //Input and output matrices
    F = (float*) malloc(maxSizeF * sizeof(float));
    inOut.buff = (float*) malloc(max(maxSizeOut,maxSizeIn) * sizeof(float));
    inOut.partner = (struct dualBufferS *) malloc(sizeof(struct dualBufferS));
    inOut.partner->buff = (float*) malloc(max(maxSizeOut,maxSizeIn) * sizeof(float));
    inOut.partner->partner = &inOut;
    if(F == NULL || inOut.buff == NULL || inOut.partner->buff == NULL)
    {
        perror("Error allocating matrices:");
        exit(2);
    }
    //auxiliar matrices
    Ac_pack = (float*) aligned_alloc(4096,BLOCK_MC*BLOCK_KC*sizeof(float));
    Bc_pack = (float*) aligned_alloc(4096,BLOCK_KC*BLOCK_NC*sizeof(float));
    Aux = (float*) malloc(maxSizeAux * sizeof(float));
    if(Aux == NULL || Ac_pack == NULL || Bc_pack == NULL)
    {
        perror("Error allocating auxiliar matrices:");
        exit(2);
    }
	
    bli_srandv(maxSizeF,F,1);
    bli_srandv(maxSizeIn,inOut.buff,1);

    
    numBatch = (maxBatch -minBatch)/stepBatch+1;
    //Allocating timing structure
	times = (double **) malloc (timers * sizeof(double*));
    tConv = (double *) calloc ((nL+2)*numBatch,sizeof(double));
	tIm2Col = (double *) calloc ((nL+2)*numBatch,sizeof(double));
    tGemm = (double *) calloc ((nL+2)*numBatch,sizeof(double));
	tIm2ColGemm= (double *) calloc ((nL+2)*numBatch,sizeof(double));
	tImp= (double *) calloc ((nL+2)*numBatch,sizeof(double));
    if(tConv == NULL || tIm2Col == NULL || tIm2ColGemm == NULL || tImp == NULL)
    {
        perror("Error allocating timing structures:");
        exit(2);
    }
    times[0] = tConv;
	times[1] = tIm2Col;
    times[2] = tGemm;
	times[3] = tIm2ColGemm;
	times[4] = tImp;

    
    for(b=minBatch,j=0;b<=maxBatch; b+=stepBatch,j++)
    {
        printf("Evaluation with batch=%d\n",b);

#ifndef NONAIVE  
        timeNet(CONV, model, nL, repe, b, j, F, &inOut, Ac_pack, Bc_pack, Aux,  tConv);
#endif
        timeNet(GEMM, model, nL, repe, b, j, F,  &inOut, Ac_pack, Bc_pack, Aux,  tGemm);
        
        timeNet(IM2COL_GEMM, model, nL, repe, b, j, F, &inOut, Ac_pack, Bc_pack, Aux,  tIm2ColGemm);
        
        timeNet(IMPLICIT_GEMM, model, nL, repe, b, j, F, &inOut, Ac_pack, Bc_pack, Aux,  tImp);
        

#ifdef LAYER_EVAL   
		//Timing averaging and statistics
        for(l=0; l < nL;l++)
        {
            tIm2Col[l+j*(nL+2)] =  tIm2ColGemm[l+j*(nL+2)] -  tGemm[l+j*(nL+2)];//Compute im2col times
            for(i = 0; i < timers; i++)
                times[i][l+j*(nL+2)] /= repe;
        }

        for(i = 0; i < timers; i++)
        {
            bli_dasumv(nL,&times[i][j*(nL+2)],1,&sum);
            times[i][nL+j*(nL+2)] = sum;
            times[i][nL+1+j*(nL+2)]= sum/nL;
        }
#else
        tIm2Col[nL+j*(nL+2)] =  tIm2ColGemm[nL+j*(nL+2)] -  tGemm[nL+j*(nL+2)]; //Compute im2col time
        for(i = 0; i < timers; i++)
        {
            times[i][nL+j*(nL+2)] /= repe;
            times[i][nL+1+j*(nL+2)]= times[i][nL+j*(nL+2)]/nL;
        }
#endif
    }
    
    free(F);
    free(inOut.buff);
    free(inOut.partner->buff);
    free(inOut.partner);
    free(Ac_pack);
    free(Bc_pack);
    free(Aux);
    
    return times;
}

/** Performs a convolution evaluation of a DNN model.
 * 
 * Evaluates all the layers on the model for each convolution algorithm tested
 * and computes the timing measures.
 * 
 * @param[in] model CNN model parameters.
 * @param[in] nL Number of layers of the model.
 * @param[in] minBatch Minimum batch size to evaluate.
 * @param[in] maxBatch Max batch size to evaluate.
 * @param[in] stepBatch Step for the batch size range.
 * @param[in] repe Number of repetitios for timing smoothing.
 * @return Matrix stack containing all the timing measurments( timing_measures * (nL * numbatch)).
 */
double ** evalNet_precision(int** model, const int nL, const int minBatch, const int maxBatch, const int stepBatch, const int repe)
{
    double **times, tIni, *tHalf, *tSingle; //Timing vectors
    double sum;
    int i, j,r,l, b, //loop indexes
     	maxSizeF, maxSizeIn, maxSizeOut, maxSizeAux;//matrixz max sizes
    int numBatch, timers=2;
    int h, w, c, kh, kw, kn, stride, pad, ho, wo;//convolution parameters
    float *F, *Ac_pack, *Bc_pack, *Aux;
    _Float16 *hF, *hAc_pack, *hBc_pack, *hAux;
    struct dualBufferS inOut, *piO; //Double buffer, and pointer for navigation
    struct dualBufferH hinOut, *hpiO; //Half precision Double buffer, and pointer for navigation
    
	
	printf("Starting evaluation\n");
    
    //Allocating matrices
    maxMatrixSizes(model, nL, maxBatch, &maxSizeF, &maxSizeIn,&maxSizeOut, &maxSizeAux);

    //Input and output matrices
    F = (float*) malloc(maxSizeF * sizeof(float));
    inOut.buff = (float*) malloc(max(maxSizeOut,maxSizeIn) * sizeof(float));
    inOut.partner = (struct dualBufferS *) malloc(sizeof(struct dualBufferS));
    inOut.partner->buff = (float*) malloc(max(maxSizeOut,maxSizeIn) * sizeof(float));
    inOut.partner->partner = &inOut;
    piO = &inOut;
    if(F == NULL || inOut.buff == NULL || inOut.partner->buff == NULL)
    {
        perror("Error allocating matrices:");
        exit(2);
    }
    //auxiliar matrices
    Ac_pack = (float*) aligned_alloc(4096,BLOCK_MC*BLOCK_KC*sizeof(float));
    Bc_pack = (float*) aligned_alloc(4096,BLOCK_KC*BLOCK_NC*sizeof(float));
    if(Ac_pack == NULL || Bc_pack == NULL)
    {
        perror("Error allocating auxiliar matrices:");
        exit(2);
    }
    
    //Input and output matrices (half precision)
    hF = (_Float16*) malloc(maxSizeF * sizeof(_Float16));
    hinOut.buff = (_Float16*) malloc(max(maxSizeOut,maxSizeIn) * sizeof(_Float16));
    hinOut.partner = (struct dualBufferH *) malloc(sizeof(struct dualBufferH));
    hinOut.partner->buff = (_Float16*) malloc(max(maxSizeOut,maxSizeIn) * sizeof(_Float16));
    hinOut.partner->partner = &hinOut;
    hpiO = &hinOut;
    if(hF == NULL || hinOut.buff == NULL || hinOut.partner->buff == NULL)
    {
        perror("Error allocating matrices:");
        exit(2);
    }
    //auxiliar matrices
    hAc_pack = (_Float16*) aligned_alloc(4096,hBLOCK_MC*hBLOCK_KC*sizeof(_Float16));
    hBc_pack = (_Float16*) aligned_alloc(4096,hBLOCK_KC*hBLOCK_NC*sizeof(_Float16));
    if(hAc_pack == NULL || hBc_pack == NULL)
    {
        perror("Error allocating auxiliar matrices:");
        exit(2);
    }
	
    bli_srandv(maxSizeF,F,1);
    decreasePrecissionV_SH(maxSizeF,F,hF);
    bli_srandv(maxSizeIn,inOut.buff,1);
    decreasePrecissionV_SH(maxSizeIn,inOut.buff,hinOut.buff);

    
    numBatch = (maxBatch -minBatch)/stepBatch+1;
    //Allocating timing structure
	times = (double **) malloc (timers * sizeof(double*));
    tHalf = (double *) calloc (numBatch,sizeof(double));
	tSingle = (double *) calloc (numBatch,sizeof(double));
    if(tSingle == NULL || tHalf == NULL)
    {
        perror("Error allocating timing structures:");
        exit(2);
    }
	times[0] = tSingle;
    times[1] = tHalf;


    
    for(b=minBatch,j=0;b<=maxBatch; b+=stepBatch,j++)
    {
        printf("Evaluation with batch=%d\n",b);

        //Evaluating simple precision convGemm
        tIni = bli_clock();
        for(r = 0; r < repe; r++)
            for(l=0; l < nL; l++)
            {
                h = model[l][1]; w = model[l][2]; c = model[l][3];
                kh = model[l][4]; kw = model[l][5]; kn = model[l][6];
                stride = model[l][7]; pad = model[l][8];
                ho = floor((h - kh + 2 * pad) / stride + 1);
                wo = floor((w - kw + 2 * pad) / stride + 1);

                sgemm_conv(kh,kw,c,kn,1,F, ho,wo,b, stride, piO->buff, 0,piO->partner->buff,Ac_pack,Bc_pack);
                piO = piO->partner;

                
            }              
        tSingle[j] = (bli_clock() -tIni )/repe;

        //Evaluating half precision convGemm
        tIni = bli_clock();
        for(r = 0; r < repe; r++)
            for(l=0; l < nL; l++)
            {
                h = model[l][1]; w = model[l][2]; c = model[l][3];
                kh = model[l][4]; kw = model[l][5]; kn = model[l][6];
                stride = model[l][7]; pad = model[l][8];
                ho = floor((h - kh + 2 * pad) / stride + 1);
                wo = floor((w - kw + 2 * pad) / stride + 1);

                hgemm_conv(kh,kw,c,kn,1,hF, ho,wo,b, stride, hpiO->buff, 0,hpiO->partner->buff,hAc_pack,hBc_pack);
                hpiO = hpiO->partner;

            }              
        tHalf[j] = (bli_clock() -tIni )/repe;
        
    }
    
    free(F);
    free(inOut.buff);
    free(inOut.partner->buff);
    free(inOut.partner);
    free(Ac_pack);
    free(Bc_pack);
    
    free(hF);
    free(hinOut.buff);
    free(hinOut.partner->buff);
    free(hinOut.partner);
    free(hAc_pack);
    free(hBc_pack);

    
    return times;
}

/** Loads model from file.
 * 
 * Loads model parameters from file and generates model structure. 
 * The model array generated contains an integer matrix of number_layers x 9.
 * 
 * @param[in] modelName Name of the file containing the model.
 * @param[out]  modelPtr Pointer to the model structure generated.
 * @return The number of layers of the model.
 */
int loadModel(const char* modelName, int *** modelPtr)
{
    int nL, l = 0, 
		filled;
    FILE* in;
    int **model;
	
        
    in = fopen(modelName,"r");
    if(in==NULL)
    {
        perror("Error opening model file:");
        exit(1);
    }
    
    filled = fscanf(in,"%*6c%d\n",&nL); //read number of layers
	if(filled !=1)
	{
		        perror("Error reading model file:");
				exit(1);
	}
	
    model = (int**) malloc(nL*sizeof(int*));//Alloc model structure
    
    filled = fscanf(in,"%*[^\n]\n");//skip descriptor line
    for(l=0; l < nL; l++)
    {
        model[l] = (int*) malloc(9 * sizeof(int)); //Alloc layer product dimensions
        //model = [id,h,w,c,kh,kw,kn,stride,pad]
		filled = fscanf(in,"%d;%d;%d;%d;%d;%d;%d;%d;%d#\n",&model[l][0],&model[l][1],&model[l][2],&model[l][3],&model[l][4],&model[l][5],&model[l][6],&model[l][7],&model[l][8]);
		if(filled !=9)
		{
			perror("Error reading layer:");
			exit(1);
		}
    }
    
    fclose(in);
    
    *modelPtr = model;
    
    printf("Model %s loaded[%d layers]\n",modelName,nL);
    
    return nL;
}


/** Prints evaluation results  to standard output.
 * 
 * Formats and prints the timing measures obtained by the evaluation. 
 * Furthermore it computes performance metrics in Gflops to enrich the 
 * presented results.
 * 
 * 
 * @param[in] nL Number of layers in the model.
 * @param[in] model CNN model parameters.
 * @param[in] perfMeasures Matrix containing the timing measures for each layer and each convolution method.
 * @param[in] minBatch Minimum batch size used in the evaluation.
 * @param[in] maxBatch Maximum batch sizeused in the evaluation.
 * @param[in] stepBatch Step for batch size used in the evaluation.
 * @param[in] repe NUmber of repetitions used to smooth the results.
 */
void genOutput(const int nL, int ** model, double** perfMeasures,const int minBatch, const int maxBatch, const int stepBatch,const int repe)
{
    int i, l,j,b;
	int id, h,w,c,kh,kw,kn,stride,pad,ho,wo;
	double gflop, gflopTotal;
	
	
    printf("Evaluation results (averaged results of %d repetitions):\n",repe);
    for(b=minBatch,j=0;b<=maxBatch; b+=stepBatch,j++)
    {
        printf("Batch=%d \n",b);
		gflopTotal=0;
		printf("layer \t Naive    \t im2col    \t gemm    \t GflopsGemm    \t im2colgemm   \t GflopsIm2ColGemm  \t implicitGemm  \t GflopsImplicit\n");

        for (l = 0;l < nL; l++)
		{
				id = model[l][0];
				h = model[l][1]; w = model[l][2]; c = model[l][3];
				kh = model[l][4]; kw = model[l][5]; kn = model[l][6];
				stride = model[l][7]; pad = model[l][8];
				ho = floor((h - kh + 2 * pad) / stride + 1);
				wo = floor((w - kw + 2 * pad) / stride + 1);
				
				gflop = ( 2.0 * kn*ho*wo*b*kh*kw*c ) /  1.0e9 ;
				gflopTotal+=gflop;
#ifdef LAYER_EVAL  
				printf("%d    \t %.4g    \t %.4g    \t %.4g    \t %.5g    \t %.4g  \t %.5g         \t %.4g    \t %.5g\n",
				       id,perfMeasures[0][l+j*(nL+2)],perfMeasures[1][l+j*(nL+2)],
					   perfMeasures[2][l+j*(nL+2)], gflop / perfMeasures[2][l+j*(nL+2)],
                       perfMeasures[3][l+j*(nL+2)], gflop/perfMeasures[3][l+j*(nL+2)],
					   perfMeasures[4][l+j*(nL+2)], gflop/perfMeasures[4][l+j*(nL+2)]);
#endif	
			
		}

	
		printf("Tot \t %.4g    \t %.4g    \t %.4g    \t %.5g    \t %.4g  \t %.5g           \t %.4g    \t %.5g\n",
				       perfMeasures[0][l+j*(nL+2)],perfMeasures[1][l+j*(nL+2)],
					   perfMeasures[2][l+j*(nL+2)], gflopTotal / perfMeasures[2][l+j*(nL+2)],
                       perfMeasures[3][l+j*(nL+2)], gflopTotal /perfMeasures[3][l+j*(nL+2)],
					   perfMeasures[4][l+j*(nL+2)], gflopTotal /perfMeasures[4][l+j*(nL+2)]);
		l++;
		printf("Avg \t %.4g    \t %.4g    \t %.4g    \t %.5g    \t %.4g   \t %.5g          \t %.4g    \t %.5g\n",
				       perfMeasures[0][l+j*(nL+2)],perfMeasures[1][l+j*(nL+2)],
					   perfMeasures[2][l+j*(nL+2)], (gflopTotal/nL) / perfMeasures[2][l+j*(nL+2)],
                       perfMeasures[3][l+j*(nL+2)], (gflopTotal/nL) /perfMeasures[3][l+j*(nL+2)],
					   perfMeasures[4][l+j*(nL+2)], (gflopTotal/nL) /perfMeasures[4][l+j*(nL+2)]);
    }
    
}


/** Prints evaluation results  to standard output.
 * 
 * Formats and prints the timing measures obtained by the evaluation. 
 * Furthermore it computes performance metrics in Gflops to enrich the 
 * presented results.
 * 
 * 
 * @param[in] nL Number of layers in the model.
 * @param[in] model CNN model parameters.
 * @param[in] perfMeasures Matrix containing the timing measures for each layer and each convolution method.
 * @param[in] minBatch Minimum batch size used in the evaluation.
 * @param[in] maxBatch Maximum batch sizeused in the evaluation.
 * @param[in] stepBatch Step for batch size used in the evaluation.
 * @param[in] repe NUmber of repetitions used to smooth the results.
 */
void genOutput_precision(const int nL, int ** model, double** perfMeasures,const int minBatch, const int maxBatch, const int stepBatch,const int repe)
{
    int l,j,b;
	int id, h,w,c,kh,kw,kn,stride,pad,ho,wo;
	double gflop, gflopTotal;
	
	
    printf("Evaluation results (averaged results of %d repetitions):\n",repe);
    
#ifdef out_csv
    printf("batch;single_time;single_performance;half_time;half_performance\n");
#endif
    
    for(b=minBatch,j=0;b<=maxBatch; b+=stepBatch,j++)
    {
		gflopTotal=0;
        for (l = 0;l < nL; l++)
		{
				id = model[l][0];
				h = model[l][1]; w = model[l][2]; c = model[l][3];
				kh = model[l][4]; kw = model[l][5]; kn = model[l][6];
				stride = model[l][7]; pad = model[l][8];
				ho = floor((h - kh + 2 * pad) / stride + 1);
				wo = floor((w - kw + 2 * pad) / stride + 1);
				
				gflop = ( 2.0 * kn*ho*wo*b*kh*kw*c ) /  1.0e9 ;
				gflopTotal+=gflop;
		}

#ifdef out_csv
    printf("%d;%.4f;%.3f;%.3f;%.4f;%.3f;%.3f\n", b, perfMeasures[0][j],gflopTotal/perfMeasures[0][j],b/perfMeasures[0][j], perfMeasures[1][j],gflopTotal/perfMeasures[1][j],b/perfMeasures[1][j]);
#else
		printf("batch[%d],single[T=%.4f,P=%.3f,S=%.3f],half[T=%.4f,P=%.3f, S=%.3f]\n", b, perfMeasures[0][j],gflopTotal/perfMeasures[0][j],b/perfMeasures[0][j], perfMeasures[1][j],gflopTotal/perfMeasures[1][j],b/perfMeasures[1][j]);
#endif
    }
}

int main( int argc, char** argv )
{
    char* modelName;    
    int minBatch,
        maxBatch,
        stepBatch,
        repe,
        nL;
    int ** model;
    double** perfMeasures;
    
    
    if (argc != 6)
    {
        printf("Performs an evaluation of the convolution layers involved in a DNN inference.\n");
        printf("\tmodel: file containing the network product structure.\n");
        printf("\tminBatch: minimum batch to test the network .\n");
        printf("\tmaxBatch: maximum batch to test the network.\n");
        printf("\tstepBatch: step for the batch performance evaluation.\n");
        printf("\trepe: number of repetitions of the test.\n");
        printf("Usage: %s <model> <minBatch> <maxBatch> <stepBatch> <repe>\n", argv[0]);
        return -1;
    }
    modelName = argv[1];
    minBatch =atoi(argv[2]);  
    maxBatch =atoi(argv[3]);
    stepBatch = atoi(argv[4]);
    repe  =atoi(argv[5]);

    
    nL = loadModel(modelName, &model);
   
#ifdef eval_precision
    perfMeasures = evalNet_precision(model, nL, minBatch, maxBatch, stepBatch, repe);
    
    genOutput_precision(nL, model, perfMeasures, minBatch, maxBatch, stepBatch, repe);
#else
    perfMeasures = evalNet(model, nL, minBatch, maxBatch, stepBatch, repe);
    
    genOutput(nL, model, perfMeasures, minBatch, maxBatch, stepBatch, repe);
#endif
    
    free(perfMeasures);
    free(model);
}
